{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 04 \n",
    "### Introduction to Data Science EN.553.436/EN.553.636 - Fall 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression and LASSO Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Below we load the [Boston house prices dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset). We store the labels of predictors for you and split the dataset into a training and test set using 1/3 as the test size and a random state of 553."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_bunch = load_boston()\n",
    "X = boston_bunch.data\n",
    "y = boston_bunch.target\n",
    "labels = boston_bunch.feature_names\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=553)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "In the homework1, we built three different linear models by OLS to predict house price (MEDV) and calculated their $R^{2}$'s. The models of using all the 13 predictor variables and using the polynomial combinations of the 13 predictor variables both have high $R^{2}$'s. But is it optimal to use all the predictor variables? Compute the variance, bias and the Mean Squared Error (MSE) of three models we built in the homework1. What can be observed?\n",
    "\n",
    "(Import the function bias_variance_decomp() from mlxtend.evaluate to calculate the variance, bias and the MSE.\n",
    "You may see more details here: http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/#api. \n",
    "And the source code of the function is here:https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/bias_variance_decomp.py )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# MODEL 1: using all predictor variables\n",
    "reg1 = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# MODEL 2: using only AGE, NOX, DIS, and RAD as predictor variables\n",
    "ind2 = np.where([a in ['AGE','NOX','DIS','RAD'] for a in boston_bunch.feature_names])[0]\n",
    "reg2 = LinearRegression().fit(X_train[:,ind2], y_train)\n",
    "\n",
    "# MODEL 3: using all polynomial combinations of degree  â‰¤2  of the original thirteen predictor variables\n",
    "import sklearn.preprocessing as prepro\n",
    "poly = prepro.PolynomialFeatures(2)\n",
    "X_train_enhanced = poly.fit_transform(X_train)\n",
    "X_test_enhanced = poly.fit_transform(X_test)\n",
    "reg3 = LinearRegression().fit(X_train_enhanced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "In fact, as we add more and more parameters to our model, its complexity increases, which results in increasing variance and decreasing bias, i.e., overfitting. See the picture: <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/05153332/model-complex.png\" alt=\"image info\" /> So we need to balance the variance and the bias. In practice, we usually use regularization to overcome overfitting.\n",
    "\n",
    "Implement Ridge Regression and LASSO Regression on the enhanced dataset. Compute the $R^{2}$, variance, bias and MSE. Which one performs better?.(use alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Now implement Ridge Regression and LASSO Regression on the original dataset. Change the values of the hyperparameters of Ridge and LASSO, how does the magnitude of the coefficients change? Is there any difference between these two methods? If we have a large dataset with 10,000 features, and some of the independent features are correlated with other independent features, which regression would you use, Ridge or LASSO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "For the Boston house prices dataset, split the dataset into a training and test set using 2/5 as the test size and a random state of 553, and use polynomial features of degree 3, then run standard LinearRegression. Can you interpret the resulting test and training error in the context of the bias-variance tradeoff? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "Apply PCA with a number of 5, 50, 100 and 200 principal component, and run LinearRegression subsequently on the resulting principal components. What can be observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
