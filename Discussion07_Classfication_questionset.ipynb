{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 07 - Classfication\n",
    "### Introduction to Data Science EN.553.436/EN.553.636 - Fall 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score,classification_report,mean_squared_error,confusion_matrix, r2_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Classification is the task of approximating a mapping function (f) from input variables (X) to **discrete output variables (y)**. The output variables are often called `labels` or `categories`. A classification problem with two classes is often called binary classification problem. A problem with more than two classes is often called a multi-class classification problem.\n",
    "\n",
    "For example, an email of text can be classified as belonging to one of two classes: “spam“ and “not spam“;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs Regression \n",
    "\n",
    "Recap: Regression is the task of approximating a mapping function (f) from input variables (X) to a **continuous output variable (y)**. A continuous output variable is a real-value, such as an integer or floating point value.\n",
    "\n",
    "| Classification | Regression |\n",
    "| --- | --- |\n",
    "| discrete | countious|\n",
    "| In Regression, we try to find the best fit line, which can predict the output more accurately. | In Classification, we try to find the decision boundary, which can divide the dataset into different classes.|\n",
    "| Classification predictions can be evaluated using accuracy, whereas regression predictions cannot.|Regression predictions can be evaluated using mean squared error, whereas classification predictions cannot.\n",
    "\n",
    "<img src=https://static.javatpoint.com/tutorial/machine-learning/images/regression-vs-classification-in-machine-learning.png alt=\"image info\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "\n",
    "Due to spread of COVID-19, vaccine development is being demanded as soon as possible. The dataset we use in this notebook describes the [B-cell](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiDidjy38fzAhX5oHIEHVEOC0IQFnoECAwQAw&url=https%3A%2F%2Fwww.hindawi.com%2Fjournals%2Fjir%2F2017%2F2680160%2F&usg=AOvVaw1qjR4h3uzKaJu1p3eWWYIW) epitope predictions, which is the antigen portion binding to the immunoglobulin or antibody. \n",
    "\n",
    "\n",
    "`input_bcell.csv` : this is our main training data. It has 14387 rows and 14 columns.\n",
    "\n",
    "Columns Interpretions:\n",
    "* `parent_protein_id`: parent protein ID\n",
    "* `protein_seq`: parent protein sequence\n",
    "* `start_position`: start position of peptide\n",
    "* `end_position`: end position of peptide\n",
    "* `peptide_seq`: peptide sequence\n",
    "* `chou_fasman`: peptide feature, β turn\n",
    "* `emini`: peptide feature, relative surface accessibility\n",
    "* `kolaskar_tongaonkar`: peptide feature, antigenicity\n",
    "* `parker`: peptide feature, hydrophobicity\n",
    "* `isoelectric_point`: protein feature\n",
    "* `aromacity`: protein feature\n",
    "* `hydrophobicity`: protein feature\n",
    "* `stability`: protein feature\n",
    "* `target`: antibody valence (target value containing 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***KNN*** calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. Then it selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong.\n",
    "<img src= https://s3.amazonaws.com/stackabuse/media/k-nearest-neighbors-algorithm-python-scikit-learn-2.png alt=\"image info\" style=\"width: 300px;\"/>\n",
    "\n",
    "For instance, we want to classify a new point 'X' belongs to Blue class or Red class. We can use KNN with number of neighbors equal to `3`, where it means finds the `3 nearest points` with least distance to point X. KNN first calcualtes the distance btween X and all other points, then pick 3 nearest points (circled above). Since there are two Red points and one Blue point inside the circle, then we choose Red to be the class of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the dataset, create a correlation matrix, split the data with 30% testing, and apply KNN to do classification with number of neighbors equal to 3. What is the prediction accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to use different number of neighbors, such as [1,3,4,6,10,30,50] in KNN classifier and plot the prediction accuracy. What can you observe? How to choose number of neighbors in KNN?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see different number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a line graph with above result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LDA*** is a `dimensionality reduction` technique. It reduces the number of dimensions (i.e. variables or dimensions or features) in a dataset while retaining as much information as possible. LDA works by calculating summary statistics for the input features by class label, such as the mean and standard deviation. It uses those information to create a new axis and `projects the data` on to the new axis in such a way as to minimizes the variance and maximizes the distance between the means of the two classes.\n",
    "\n",
    "***Comparing LDA and PCA:***\n",
    "<img src=https://nirpyresearch.com/wp-content/uploads/2018/11/PCAvsLDA-1024x467.png alt=\"image info\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply LDA to do classification. What is the prediction accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Naive Bayes*** is a classification algorithm that works based on the Bayes theorem. If we are trying to decide between two labels using Naive Bayes classifier, then we can compute the ratio of the posterior probabilities for each label. Then assign the new point to most probable class. It assumes `independence` among predictors.\n",
    "\n",
    "***General Steps:***\n",
    "* Step 1: Calculate the prior probability for each class\n",
    "* Step 2: Find Likelihood probability with each attribute for each class\n",
    "* Step 3: Calculate posterior probability using Bayes Theorem\n",
    "* Step 4: Make prediction, choose the class with highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you manually compute predictions and prediction accuracy by using Gaussian Naive Bayes method?**\n",
    "* Step1, calculate mean and std for two classes (target = 0 and target = 1)\n",
    "* Step2, compute likelyhood by using gaussian function\n",
    "* Step3, compute posterior\n",
    "* Step4, compare posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Gaussian Naive Bayes to do classification. What is the prediction accuracy? Why it's higher/lower compare to pervious two models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gussian naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 - Combine Normalization and PCA with Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine methods we learnt before with this classfication problem. First apply normalization to KNN and LDA models. Then apply both normalization and PCA to those two models. Do Normalization or PCA help increase the accuracy score? Why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data and apply PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "document:\n",
    "\n",
    "https://www.javatpoint.com/regression-vs-classification-in-machine-learning\n",
    "\n",
    "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/\n",
    "\n",
    "https://medium.com/machine-learning-algorithms-from-scratch/naive-bayes-classification-from-scratch-in-python-e3a48bf5f91a\n",
    "\n",
    "data: \n",
    "\n",
    "https://future-architect.github.io/articles/20200801/\n",
    "\n",
    "https://www.kaggle.com/futurecorporation/epitope-prediction?select=input_bcell.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
