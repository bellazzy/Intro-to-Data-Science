{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4f7dfb",
   "metadata": {},
   "source": [
    "# Section 08 - More Classification\n",
    "### Introduction to Data Science EN.553.436/EN.553.636 - Fall 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfe970",
   "metadata": {},
   "source": [
    "For this analysis we will use the phoneme data set from OpenML. Read the full description [here](https://www.openml.org/d/1489), but key details are below:\n",
    "\n",
    "Features:\n",
    "- V1: Amplitude of first harmonic\n",
    "- V2: Amplitude of second harmonic\n",
    "- V3: Amplitude of third harmonic\n",
    "- V4: Amplitude of fourth harmonic\n",
    "- V5: Amplitude of fifth harmonic\n",
    "\n",
    "Classes:\n",
    "- 1 - Nasal vowel\n",
    "- 2 - Oral vowel\n",
    "\n",
    "You will first need to download the .csv file on blackboard, and save it in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dec3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "vowel_df = pd.read_csv(\"vowel_data.csv\")\n",
    "display(vowel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd65869",
   "metadata": {},
   "source": [
    "To make this data easier to graph and visualize, we will only work with the first two features and drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3364de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vowel_df[['V1','V2']])\n",
    "y = np.array(vowel_df['Class'])\n",
    "print('Features 1 and 2: \\n')\n",
    "print(X)\n",
    "print('\\n Class:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191c689",
   "metadata": {},
   "source": [
    "# 1. Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f159c7",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "**Use a 30% train-test split and implement QDA and LDA on the same data (Use random state 235). Compare the accuracy scores between QDA and LDA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train,X_test,y_train,y_test = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce4f1f",
   "metadata": {},
   "source": [
    "## 1.2 \n",
    "**Compare the prediction results from QDA and LDA. Determine the number of test data points for which QDA and LDA predicted different classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd50646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdcb2a81",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "When we are dealing with 2 classes, we can plot a decision boundary where the probability of Class 1 is equal to the probability of Class 2. This concept becomes much less straightforward when dealing with more than 2 classes.\n",
    "\n",
    "The plot below will show the decision boundaries for QDA and LDA (Which one is which?), along with the actual classes for each of the test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Update these lines\n",
    "QDA_model = \n",
    "LDA_model = \n",
    "###\n",
    "\n",
    "\n",
    "x_min = -3\n",
    "x_max = 4\n",
    "y_min = -5\n",
    "y_max = 4\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "\n",
    "Z_QDA = QDA_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_QDA = Z_QDA[:, 1].reshape(xx.shape)\n",
    "\n",
    "Z_LDA = LDA_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_LDA = Z_LDA[:,1].reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z_QDA, [0.50], linewidths=2, colors='black') ##Draw the boundary where P = 0.50\n",
    "plt.contour(xx, yy, Z_LDA, [0.50], linewidths=2, colors ='black') ## Draw the boundary where P = 0.50\n",
    "\n",
    "plt.plot(X_test[y_test==1,0],X_test[y_test==1,1],'.b')\n",
    "plt.plot(X_test[y_test==2,0],X_test[y_test==2,1],'.r')\n",
    "\n",
    "plt.xlabel('V1')\n",
    "plt.ylabel('V2')\n",
    "plt.legend(['Class 1','Class 2'])\n",
    "plt.show()\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b7291",
   "metadata": {},
   "source": [
    "**Create a new plot using the same decision boundaries, but this time add the following points from your test set (with different markers):**\n",
    "- Both LDA and QDA predict class 1\n",
    "- Both LDA and QDA predict class 0\n",
    "- LDA and QDA have different predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05b17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dda9551a",
   "metadata": {},
   "source": [
    "# 2. Binary Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268fce4",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "**Using the same train and test data sets, classify the data using a binary decision tree with maximum depth 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cbc98",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "The following cell plots the decision boundaries of your binary tree. \n",
    "**Observe the differences between the binary tree, LDA, and QDA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update this line\n",
    "Tree = \n",
    "###\n",
    "\n",
    "Z_Tree = Tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_Tree = Z_Tree.reshape(xx.shape)\n",
    "plt.contourf(xx,yy,Z_Tree)\n",
    "plt.plot(X_test[y_test==1,0],X_test[y_test==1,1],'.b')\n",
    "plt.plot(X_test[y_test==2,0],X_test[y_test==2,1],'.y')\n",
    "plt.xlabel('V1')\n",
    "plt.ylabel('V2')\n",
    "plt.legend(['Class 1','Class 2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df554596",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "Gini is one option for evaluating whether one decision node leads to more purity than another.\n",
    "\n",
    "The Gini for a node with $K$ classes is defined by\n",
    "$H(D) = \\sum_{i=1}^K p_i (1-p_i)$\n",
    "\n",
    "Here $p_i$ represents the probability of a data point in that leaf node belonging to class $i$.\n",
    "To evaluate a split, we calculate the sum of the Ginis for the two leaf nodes. Gini is a measure of impurity so our goal is to\n",
    "**minimize** the Gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c9f13",
   "metadata": {},
   "source": [
    "**Determine the impurity in the original (training) dataset. Compare this to the sklearn calculation in the depth-1 tree below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58262660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sklearn Gini calculation\n",
    "Tree_One = tree.DecisionTreeClassifier(max_depth=1)\n",
    "Tree_One.fit(X_train,y_train)\n",
    "print('(sklearn) Gini Impurity in original data set: ' + str(Tree_One.tree_.impurity[0]))\n",
    "\n",
    "## manual Gini calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd6435",
   "metadata": {},
   "source": [
    "## 2.4 \n",
    "We can use cross validation to determine the best parameters to use for our classification model. For the case of binary trees, it can be used to determine what maximum depth to use.\n",
    "\n",
    "**For depths from 1 to 15, perform 10-fold cross validation on your binary tree, using your full data set. Calculate and plot the mean accuracy for each depth.**\n",
    "**What is the ideal maximum tree depth for our data? What do you observe as the depth increases to 20? 30?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
